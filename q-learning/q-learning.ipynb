{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerp(start, end, x):\n",
    "    return (x * end) + ((1 - x) * start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class MathHelperTest(unittest.TestCase):\n",
    "    \n",
    "    def test_lerp_0(self):\n",
    "        self.assertEqual(lerp(-2, 2, 0), -2)\n",
    "    \n",
    "    def test_lerp_25(self):\n",
    "        self.assertEqual(lerp(-2, 2, 0.25), -1)\n",
    "    \n",
    "    def test_lerp_75(self):\n",
    "        self.assertEqual(lerp(-2, 2, 0.75), 1)\n",
    "        \n",
    "    def test_lerp_1(self):\n",
    "        self.assertEqual(lerp(-2, 2, 1), 2)\n",
    "        \n",
    "    def test_lerp_backwards(self):\n",
    "        self.assertEqual(lerp(2, -2, 0.25), 1)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay memory stores given `capacity` amount of transitions in a buffer. It provides methods for pushing new transitions and sampling a minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.clear()\n",
    "    \n",
    "    def clear(self):\n",
    "        self.memory = deque(maxlen=self.capacity)\n",
    "    \n",
    "    def push(self, transition):\n",
    "        self.memory.appendleft(transition)\n",
    "    \n",
    "    def sample(self, size, withReplacement=False):\n",
    "        if size > len(self.memory) and not withReplacement:\n",
    "            return None\n",
    "        \n",
    "        indecies = np.random.choice(len(self.memory), size, replace=withReplacement)\n",
    "        return np.array(list(self.memory))[indecies]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "............\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class ReplayMemoryTest(unittest.TestCase):\n",
    "    \n",
    "    def test_should_drop_oldest_memory_on_overflow(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        memory.push(3)\n",
    "        memory.push(4)\n",
    "        memory.push(5)\n",
    "        \n",
    "        self.assertEqual(list(memory.memory), [5, 4, 3, 2])\n",
    "        \n",
    "    def test_should_sample_with_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        memory.push(3)\n",
    "        memory.push(4)\n",
    "        \n",
    "        miniBatch = memory.sample(3, withReplacement=True)\n",
    "        self.assertEqual(len(miniBatch), 3)\n",
    "        \n",
    "        for elem in miniBatch:\n",
    "            self.assertIn(elem, [1, 2, 3, 4])\n",
    "            \n",
    "    def test_should_sample_without_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        memory.push(3)\n",
    "        memory.push(4)\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=False)\n",
    "        miniBatch.sort()\n",
    "        \n",
    "        self.assertEqual(miniBatch.tolist(), [1, 2, 3, 4])\n",
    "    \n",
    "    def test_should_sample_larger_batch_size_with_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=True)\n",
    "        \n",
    "        self.assertEqual(len(miniBatch), 4)\n",
    "        for elem in miniBatch:\n",
    "            self.assertIn(elem, [1, 2])\n",
    "    \n",
    "    def test_should_return_none_when_spamling_larger_batch_size_without_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=False)\n",
    "        self.assertEqual(miniBatch, None)\n",
    "    \n",
    "    def test_should_handle_memorizing_multidimensional_arrays(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push([1, 2])\n",
    "        memory.push([3, 4])\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=True)\n",
    "        \n",
    "        self.assertEqual(len(miniBatch), 4)\n",
    "        for elem in miniBatch:\n",
    "            self.assertTrue((elem == [1, 2]).all() or (elem == [3, 4]).all())\n",
    "    \n",
    "    def test_should_clear_memory(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        \n",
    "        memory.clear()\n",
    "        self.assertEqual(len(memory.memory), 0)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "ActionSelections = namedtuple('ActionSelections', ['indecies', 'values'])\n",
    "\n",
    "class EpsilonGreedyPolicy(object):\n",
    "    \n",
    "    def __init__(self, epsilon, anealing=None, random_engine=random):\n",
    "        self.epsilon = epsilon\n",
    "        self.anealing = anealing\n",
    "        self.random_engine = random_engine\n",
    "    \n",
    "    def select_action(self, action_value_functions, step=None, force=None):\n",
    "        if action_value_functions is None or len(action_value_functions) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Used at selection contains row indecies\n",
    "        row_idx = range(len(action_value_functions))\n",
    "        \n",
    "        # Find index of max column for each row\n",
    "        greedy_actions = np.argmax(action_value_functions, axis=1)\n",
    "\n",
    "        # Find non greedy actions for each row\n",
    "        non_greedy_actions = np.random.choice(len(action_value_functions[0]) - 1, len(action_value_functions))\n",
    "        non_greedy_actions += (non_greedy_actions >= greedy_actions)\n",
    "        \n",
    "        if force == 'exploit':\n",
    "            return ActionSelections(greedy_actions, np.array(action_value_functions)[row_idx, greedy_actions])\n",
    "        elif force == 'explore':\n",
    "            return ActionSelections(non_greedy_actions, np.array(action_value_functions)[row_idx, non_greedy_actions])\n",
    "        \n",
    "        limit = self.epsilon if step is None or self.anealing is None else self.anealing(step)\n",
    "        \n",
    "        if self.random_engine.uniform(0, 1) > limit:\n",
    "            return ActionSelections(greedy_actions, np.array(action_value_functions)[row_idx, greedy_actions])\n",
    "        \n",
    "        return ActionSelections(non_greedy_actions, np.array(action_value_functions)[row_idx, non_greedy_actions])\n",
    "                                                                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...................\n",
      "----------------------------------------------------------------------\n",
      "Ran 19 tests in 0.016s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "import unittest\n",
    "\n",
    "class EpsilonGreedyPolicyTest(unittest.TestCase):\n",
    "    \n",
    "    def test_should_select_greedy_when_forced(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.01)\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]], force='exploit')\n",
    "        np.testing.assert_allclose(selection.indecies, [1, 0])\n",
    "        np.testing.assert_allclose(selection.values, [0.4, 0.9])\n",
    "        \n",
    "    def test_should_select_explore_when_forced(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.01)\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]], force='explore')\n",
    "        \n",
    "        self.assertIn(selection.indecies[0], [0, 2])\n",
    "        self.assertIn(selection.indecies[1], [1, 2])\n",
    "        \n",
    "        self.assertIn(selection.values[0], [0.1, 0.2])\n",
    "        self.assertIn(selection.values[1], [0.5, 0.1])\n",
    "    \n",
    "    def test_should_select_greedy_when_random_is_higher_than_epsilon(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.1, random_engine=SimpleNamespace(uniform=lambda a, b: 0.2))\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]])\n",
    "        np.testing.assert_allclose(selection.indecies, [1, 0])\n",
    "        np.testing.assert_allclose(selection.values, [0.4, 0.9])\n",
    "    \n",
    "    def test_should_select_eploratively_when_random_is_higher_than_epsilon(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.1, random_engine=SimpleNamespace(uniform=lambda a, b: 0.05))\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]])\n",
    "        \n",
    "        self.assertIn(selection.indecies[0], [0, 2])\n",
    "        self.assertIn(selection.indecies[1], [1, 2])\n",
    "        \n",
    "        self.assertIn(selection.values[0], [0.1, 0.2])\n",
    "        self.assertIn(selection.values[1], [0.5, 0.1])\n",
    "\n",
    "    def test_should_return_none_with_empty_action_value_functions(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.01)\n",
    "        self.assertEqual(e_greedy.select_action([], force='exploit'), None)\n",
    "        \n",
    "    def test_should_use_anealing_when_step_is_provided(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.1, random_engine=SimpleNamespace(uniform=lambda a, b: 0.4), anealing=lambda step: lerp(1.0, 0.0, step / 10))\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]], step=5)\n",
    "        \n",
    "        self.assertIn(selection.indecies[0], [0, 2])\n",
    "        self.assertIn(selection.indecies[1], [1, 2])\n",
    "        \n",
    "        self.assertIn(selection.values[0], [0.1, 0.2])\n",
    "        self.assertIn(selection.values[1], [0.5, 0.1])\n",
    "    \n",
    "    def test_should_ignore_step_if_anealing_not_provided(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.1, random_engine=SimpleNamespace(uniform=lambda a, b: 0.2))\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]], step=5)\n",
    "        \n",
    "        np.testing.assert_allclose(selection.indecies, [1, 0])\n",
    "        np.testing.assert_allclose(selection.values, [0.4, 0.9])\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def env_loop(env, agent, episodes, render=False, max_steps_per_episode=None, collect_actions=False):\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    \n",
    "    for i_episode in tqdm(range(episodes), desc='episode'):\n",
    "        episode_rewards = 0\n",
    "        episode_actions = []\n",
    "        \n",
    "        # Initialize the environment and state for the episode\n",
    "        state_t = env.reset()\n",
    "        \n",
    "        i_episode_step = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done and (max_steps_per_episode is None or max_steps_per_episode > i_episode_step):\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            action_t = agent.select_action(state_t)\n",
    "            state_t1, reward_t, done, _ = env.step(action_t)\n",
    "            \n",
    "            agent.observe([state_t, action_t, reward_t, state_t1, 0 if done else 1])\n",
    "            \n",
    "            episode_rewards += reward_t\n",
    "            \n",
    "            if collect_actions:\n",
    "                episode_actions.append(action_t)\n",
    "            \n",
    "            state_t = state_t1\n",
    "            i_episode_step += 1\n",
    "        \n",
    "        rewards.append(episode_rewards)\n",
    "        \n",
    "        if collect_actions:\n",
    "            actions.append(episode_actions)\n",
    "    \n",
    "    return np.array(rewards), actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_activation_fn = nn.Sigmoid()\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input_features):\n",
    "        hidden_output = self.hidden_layer(input_features)\n",
    "        hidden_activation = self.hidden_activation_fn(hidden_output)\n",
    "        \n",
    "        return self.output_layer(hidden_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....................\n",
      "----------------------------------------------------------------------\n",
      "Ran 20 tests in 0.014s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class DeepQNetworkTest(unittest.TestCase):\n",
    "    \n",
    "    def test_should_perform_forward_pass(self):\n",
    "        input_features = torch.randn(3, 4)\n",
    "        \n",
    "        deep_q = DeepQNetwork(4, 20, 5)\n",
    "        output = deep_q(input_features)\n",
    "        \n",
    "        self.assertEqual(output.size(), (3, 5))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "\n",
    "TrainParams = namedtuple('TrainParams', ['batch_size', 'learning_rate', 'target_update_interval', 'Optimizer'])\n",
    "PolicyParams = namedtuple('PolicyParams', ['e_greedy', 'memory', 'discount_factor', 'make_qnet'])\n",
    "\n",
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, trainParams, policyParams):\n",
    "        self.e_greedy = policyParams.e_greedy\n",
    "        self.q_net = policyParams.make_qnet()\n",
    "        \n",
    "        self.memory = policyParams.memory\n",
    "        self.discount_factor = policyParams.discount_factor\n",
    "        self.target_update_interval = trainParams.target_update_interval\n",
    "        \n",
    "        self.batch_size = trainParams.batch_size\n",
    "        self.learning_rate = trainParams.learning_rate\n",
    "        self.Optimizer = trainParams.Optimizer\n",
    "        \n",
    "        self.make_qnet = policyParams.make_qnet\n",
    "        \n",
    "        self.train_step = 0\n",
    "        self.state = 'eval'\n",
    "    \n",
    "    def set_state(self, state):\n",
    "        if self.state == state or state not in ['train', 'eval']:\n",
    "            return\n",
    "        \n",
    "        if state == 'train':\n",
    "            self._init_train()\n",
    "        \n",
    "        self.state = state\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        action_values = self.q_net(torch.tensor(state).float())\n",
    "        return self.e_greedy.select_action(\n",
    "            [action_values.detach().numpy()],\n",
    "            force='exploit' if self.state == 'eval' else None,\n",
    "            step=self.train_step if self.state == 'train' else None\n",
    "        ).indecies[0]\n",
    "    \n",
    "    def observe(self, transition):\n",
    "        # No need to do anything unless is training\n",
    "        if self.state != 'train':\n",
    "            return\n",
    "        \n",
    "        # Store observed transition to memory\n",
    "        self.memory.push(transition)\n",
    "        \n",
    "        # Sample minibatch from memory\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Don't train until has enough samples\n",
    "        if batch is None:\n",
    "            return\n",
    "        \n",
    "        # Unpack batch\n",
    "        [batch_state_t, batch_action_t, batch_reward_t, batch_state_t1, continues] = batch.T\n",
    "        batch_state_t = np.concatenate(batch_state_t).reshape((-1, 4))\n",
    "        batch_action_t = batch_action_t.astype(np.int32, copy=False)\n",
    "        batch_reward_t = batch_reward_t.astype(np.float32, copy=False)\n",
    "        batch_state_t1 = np.concatenate(batch_state_t1).reshape((-1, 4))\n",
    "        continues = np.array(continues).astype(np.float32)\n",
    "        \n",
    "        # Calculate target action values for minibatch\n",
    "        target_action_value_estimates = self.q_net_target(torch.tensor(batch_state_t1).float()).detach().numpy()\n",
    "        y = batch_reward_t + continues * self.discount_factor * self.e_greedy.select_action(target_action_value_estimates, force='exploit').values\n",
    "        \n",
    "        # Reset gradients for training step\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate action values using Q for minibatch\n",
    "        action_value_estimates = self.q_net(torch.tensor(batch_state_t).float())\n",
    "        \n",
    "        # Select action values based on the taken actions\n",
    "        x = action_value_estimates[range(len(action_value_estimates)), batch_action_t]\n",
    "        \n",
    "        # Calculate loss and update weights of Q\n",
    "        loss = self.loss_fn(x, torch.from_numpy(y))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Every target_update_interval update Q- with weights of Q\n",
    "        if self.train_step % self.target_update_interval == 0:\n",
    "            self.q_net_target.load_state_dict(self.q_net.state_dict())\n",
    "            self.q_net_target.eval()\n",
    "            \n",
    "        # Update training step\n",
    "        self.train_step += 1\n",
    "    \n",
    "    def _init_train(self):\n",
    "        # Clear memory from previous training\n",
    "        self.memory.clear()\n",
    "        \n",
    "        # Make target network, initialize its weights to match Q network weights and set it to eval mode\n",
    "        self.q_net_target = self.make_qnet()\n",
    "        self.q_net_target.load_state_dict(self.q_net.state_dict())\n",
    "        self.q_net_target.eval()\n",
    "    \n",
    "        # Loss function is MSE\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = self.Optimizer(self.q_net.parameters(), self.learning_rate)\n",
    "    \n",
    "        self.train_step = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_qnet_fn(env):\n",
    "    return lambda: DeepQNetwork(env.observation_space.shape[0], 20, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45b18f5987a49e2866a39047df1f355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='episode'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "baseline actions: [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "baseline rewards: [ 8. 10. 10.  9. 10.  9.  8. 10. 10.  8.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e37d9c8610c4327b8d54357f7e9aaee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='episode'), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-8084a1b09446>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(list(self.memory))[indecies]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d438c1f4ba8043f2991a4c3367f59c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='episode'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "preview rewards: [151. 160. 155. 153. 150. 149. 152. 157. 149. 154.]\n",
      "preview actions: [[0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0], [0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0], [0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1], [0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], [0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0], [0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0], [0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set hyper-parameters\n",
    "# Replay Memory Capacity\n",
    "N = 100\n",
    "\n",
    "train_episodes = 10000\n",
    "preview_episodes = 10\n",
    "discount_rate = 0.9\n",
    "batch_size = 12\n",
    "learning_rate = 0.001\n",
    "target_update_interval = 8\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Create agent\n",
    "trainParams = TrainParams(\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "    target_update_interval,\n",
    "    optim.RMSprop\n",
    ")\n",
    "\n",
    "policyParams = PolicyParams(\n",
    "    EpsilonGreedyPolicy(0.1, anealing=lambda step: max(lerp(1.0, 0.1, step / 5000), 0.1)),\n",
    "    ReplayMemory(N),\n",
    "    discount_rate,\n",
    "    make_qnet_fn(env)\n",
    ")\n",
    "\n",
    "agent = Agent(trainParams, policyParams)\n",
    "\n",
    "# Get agent baseline\n",
    "agent.set_state('eval')\n",
    "baseline_rewards, baseline_actions = env_loop(env, agent, preview_episodes, render=False, max_steps_per_episode=200, collect_actions=True)\n",
    "print(f'baseline actions: {baseline_actions}')\n",
    "print(f'baseline rewards: {baseline_rewards}')\n",
    "\n",
    "# Train in env environment\n",
    "agent.set_state('train')\n",
    "train_rewards, train_actions = env_loop(env, agent, train_episodes, render=False, max_steps_per_episode=200)\n",
    "\n",
    "# Visualize training\n",
    "plt.plot(list(range(len(train_rewards))), train_rewards)\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('reward')\n",
    "plt.show()\n",
    "\n",
    "# Preview learned agent\n",
    "agent.set_state('eval')\n",
    "preview_rewards, preview_actions = env_loop(env, agent, preview_episodes, render=True, max_steps_per_episode=200, collect_actions=True)\n",
    "print(f'preview rewards: {preview_rewards}')\n",
    "print(f'preview actions: {preview_actions}')\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Agents Q-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "save_dir = os.path.dirname(os.path.abspath(''))\n",
    "torch.save(agent.q_net, os.path.join(save_dir, 'q-learning', 'saves', 'qnet.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Agent with Q-Model for evaluation only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "loaded_agent = Agent(trainParams, policyParams)\n",
    "load_dir = os.path.dirname(os.path.abspath(''))\n",
    "loaded_agent.q_net = torch.load(os.path.join(load_dir, 'q-learning', 'saves', 'qnet.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5b2b7ad9cb4a758cce5231dc83bca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='episode'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "preview rewards: [160. 158. 151. 146. 152. 156. 149. 160. 148. 157.]\n",
      "preview actions: [[0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1], [0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0], [0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1], [0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1], [0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0], [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1], [0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1], [0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Preview learned agent\n",
    "loaded_agent.set_state('eval')\n",
    "preview_rewards, preview_actions = env_loop(env, loaded_agent, preview_episodes, render=True, max_steps_per_episode=200, collect_actions=True)\n",
    "print(f'preview rewards: {preview_rewards}')\n",
    "print(f'preview actions: {preview_actions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
