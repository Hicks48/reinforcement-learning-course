{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay memory stores given `capacity` amount of transitions in a buffer. It provides methods for pushing new transitions and sampling a minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.clear()\n",
    "    \n",
    "    def clear(self):\n",
    "        self.memory = deque(maxlen=self.capacity)\n",
    "    \n",
    "    def push(self, transition):\n",
    "        self.memory.appendleft(transition)\n",
    "    \n",
    "    def sample(self, size, withReplacement=False):\n",
    "        if size > len(self.memory) and not withReplacement:\n",
    "            return None\n",
    "        \n",
    "        indecies = np.random.choice(len(self.memory), size, replace=withReplacement)\n",
    "        return np.array(list(self.memory))[indecies]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.011s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class ReplayMemoryTest(unittest.TestCase):\n",
    "    \n",
    "    def test_should_drop_oldest_memory_on_overflow(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        memory.push(3)\n",
    "        memory.push(4)\n",
    "        memory.push(5)\n",
    "        \n",
    "        self.assertEqual(list(memory.memory), [5, 4, 3, 2])\n",
    "        \n",
    "    def test_should_sample_with_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        memory.push(3)\n",
    "        memory.push(4)\n",
    "        \n",
    "        miniBatch = memory.sample(3, withReplacement=True)\n",
    "        self.assertEqual(len(miniBatch), 3)\n",
    "        \n",
    "        for elem in miniBatch:\n",
    "            self.assertIn(elem, [1, 2, 3, 4])\n",
    "            \n",
    "    def test_should_sample_without_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        memory.push(3)\n",
    "        memory.push(4)\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=False)\n",
    "        miniBatch.sort()\n",
    "        \n",
    "        self.assertEqual(miniBatch.tolist(), [1, 2, 3, 4])\n",
    "    \n",
    "    def test_should_sample_larger_batch_size_with_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=True)\n",
    "        \n",
    "        self.assertEqual(len(miniBatch), 4)\n",
    "        for elem in miniBatch:\n",
    "            self.assertIn(elem, [1, 2])\n",
    "    \n",
    "    def test_should_return_none_when_spamling_larger_batch_size_without_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=False)\n",
    "        self.assertEqual(miniBatch, None)\n",
    "    \n",
    "    def test_should_handle_memorizing_multidimensional_arrays(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push([1, 2])\n",
    "        memory.push([3, 4])\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=True)\n",
    "        \n",
    "        self.assertEqual(len(miniBatch), 4)\n",
    "        for elem in miniBatch:\n",
    "            self.assertTrue((elem == [1, 2]).all() or (elem == [3, 4]).all())\n",
    "    \n",
    "    def test_should_clear_memory(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        \n",
    "        memory.clear()\n",
    "        self.assertEqual(len(memory.memory), 0)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "ActionSelections = namedtuple('ActionSelections', ['indecies', 'values'])\n",
    "\n",
    "class EpsilonGreedyPolicy(object):\n",
    "    \n",
    "    def __init__(self, epsilon, random_engine=random):\n",
    "        self.epsilon = epsilon\n",
    "        self.random_engine = random_engine\n",
    "    \n",
    "    def select_action(self, action_value_functions, force=None):\n",
    "        if action_value_functions is None or len(action_value_functions) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Used at selection contains row indecies\n",
    "        row_idx = range(len(action_value_functions))\n",
    "        \n",
    "        # Find index of max column for each row\n",
    "        greedy_actions = np.argmax(action_value_functions, axis=1)\n",
    "\n",
    "        # Find non greedy actions for each row\n",
    "        non_greedy_actions = np.random.choice(len(action_value_functions[0]) - 1, len(action_value_functions))\n",
    "        non_greedy_actions += (non_greedy_actions >= greedy_actions)\n",
    "        \n",
    "        if force == 'exploit':\n",
    "            return ActionSelections(greedy_actions, np.array(action_value_functions)[row_idx, greedy_actions])\n",
    "        elif force == 'explore':\n",
    "            return ActionSelections(non_greedy_actions, np.array(action_value_functions)[row_idx, non_greedy_actions])\n",
    "        \n",
    "        if self.random_engine.uniform(0, 1) > self.epsilon:\n",
    "            return ActionSelections(greedy_actions, np.array(action_value_functions)[row_idx, greedy_actions])\n",
    "        \n",
    "        return ActionSelections(non_greedy_actions, np.array(action_value_functions)[row_idx, non_greedy_actions])\n",
    "                                                                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "............\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.106s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "import unittest\n",
    "\n",
    "class EpsilonGreedyPolicyTest(unittest.TestCase):\n",
    "    \n",
    "    def test_should_select_greedy_when_forced(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.01)\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]], force='exploit')\n",
    "        np.testing.assert_allclose(selection.indecies, [1, 0])\n",
    "        np.testing.assert_allclose(selection.values, [0.4, 0.9])\n",
    "        \n",
    "    def test_should_select_explore_when_forced(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.01)\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]], force='explore')\n",
    "        \n",
    "        self.assertIn(selection.indecies[0], [0, 2])\n",
    "        self.assertIn(selection.indecies[1], [1, 2])\n",
    "        \n",
    "        self.assertIn(selection.values[0], [0.1, 0.2])\n",
    "        self.assertIn(selection.values[1], [0.5, 0.1])\n",
    "    \n",
    "    def test_should_select_greedy_when_random_is_higher_than_epsilon(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.1, random_engine=SimpleNamespace(uniform=lambda a, b: 0.2))\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]])\n",
    "        np.testing.assert_allclose(selection.indecies, [1, 0])\n",
    "        np.testing.assert_allclose(selection.values, [0.4, 0.9])\n",
    "    \n",
    "    def test_should_select_eploratively_when_random_is_higher_than_epsilon(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.1, random_engine=SimpleNamespace(uniform=lambda a, b: 0.05))\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]])\n",
    "        \n",
    "        self.assertIn(selection.indecies[0], [0, 2])\n",
    "        self.assertIn(selection.indecies[1], [1, 2])\n",
    "        \n",
    "        self.assertIn(selection.values[0], [0.1, 0.2])\n",
    "        self.assertIn(selection.values[1], [0.5, 0.1])\n",
    "\n",
    "    def test_should_return_none_with_empty_action_value_functions(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.01)\n",
    "        self.assertEqual(e_greedy.select_action([], force='exploit'), None)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def env_loop(env, agent, episodes, render=False, max_steps_per_episode=None):\n",
    "    rewards = []\n",
    "    \n",
    "    for i_episode in range(episodes):\n",
    "        episode_rewards = 0\n",
    "        \n",
    "        # Initialize the environment and state for the episode\n",
    "        state_t = env.reset()\n",
    "        \n",
    "        i_episode_step = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done and (max_steps_per_episode is None or max_steps_per_episode > i_episode_step):\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            action_t = agent.select_action(state_t)\n",
    "            state_t1, reward_t, done, _ = env.step(action_t)\n",
    "            \n",
    "            agent.observe([state_t, action_t, reward_t, state_t1])\n",
    "            \n",
    "            episode_rewards += reward_t\n",
    "            state_t = state_t1\n",
    "            i_episode_step += 1\n",
    "        \n",
    "        rewards.append(episode_rewards)\n",
    "    \n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_activation_fn = nn.Sigmoid()\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.output_layer_activation_fn = nn.Softmax()\n",
    "        \n",
    "    def forward(self, input_features):\n",
    "        hidden_output = self.hidden_layer(input_features)\n",
    "        hidden_activation = self.hidden_activation_fn(hidden_output)\n",
    "        \n",
    "        output_output = self.output_layer(hidden_activation)\n",
    "        return self.output_layer_activation_fn(output_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-4707718b72bb>:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output_layer_activation_fn(output_output)\n",
      ".............\n",
      "----------------------------------------------------------------------\n",
      "Ran 13 tests in 0.079s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class DeepQNetworkTest(unittest.TestCase):\n",
    "    \n",
    "    def test_should_perform_forward_pass(self):\n",
    "        input_features = torch.randn(3, 4)\n",
    "        \n",
    "        deep_q = DeepQNetwork(4, 20, 5)\n",
    "        output = deep_q(input_features)\n",
    "        \n",
    "        self.assertEqual(output.size(), (3, 5))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "\n",
    "TrainParams = namedtuple('TrainParams', ['batch_size', 'learning_rate', 'target_update_interval', 'Optimizer'])\n",
    "PolicyParams = namedtuple('PolicyParams', ['e_greedy', 'memory', 'discount_factor', 'make_qnet'])\n",
    "\n",
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, trainParams, policyParams):\n",
    "        self.e_greedy = policyParams.e_greedy\n",
    "        self.q_net = policyParams.make_qnet()\n",
    "        \n",
    "        self.memory = policyParams.memory\n",
    "        self.discount_factor = policyParams.discount_factor\n",
    "        self.target_update_interval = trainParams.target_update_interval\n",
    "        \n",
    "        self.batch_size = trainParams.batch_size\n",
    "        self.learning_rate = trainParams.learning_rate\n",
    "        self.Optimizer = trainParams.Optimizer\n",
    "        \n",
    "        self.make_qnet = policyParams.make_qnet\n",
    "        \n",
    "        self.state = 'eval'\n",
    "    \n",
    "    def set_state(self, state):\n",
    "        if self.state == state or state not in ['train', 'eval']:\n",
    "            return\n",
    "        \n",
    "        if state == 'train':\n",
    "            self._init_train()\n",
    "        \n",
    "        self.state = state\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        action_values = self.q_net(torch.tensor(state).float())\n",
    "        return self.e_greedy.select_action([action_values.detach().numpy()], force='exploit').indecies[0]\n",
    "    \n",
    "    def observe(self, transition):\n",
    "        # No need to do anything unless is training\n",
    "        if self.state != 'train':\n",
    "            return\n",
    "        \n",
    "        # Store observed transition to memory\n",
    "        self.memory.push(transition)\n",
    "        \n",
    "        # Sample minibatch from memory\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Don't train until has enough samples\n",
    "        if batch is None:\n",
    "            return\n",
    "        \n",
    "        # Unpack batch\n",
    "        [batch_state_t, batch_action_t, batch_reward_t, batch_state_t1] = batch.T\n",
    "        batch_state_t = np.concatenate(batch_state_t).reshape((-1, 4))\n",
    "        batch_action_t = batch_action_t.astype(np.int32, copy=False)\n",
    "        batch_reward_t = batch_reward_t.astype(np.float32, copy=False)\n",
    "        batch_state_t1 = np.concatenate(batch_state_t1).reshape((-1, 4))\n",
    "        \n",
    "        # Calculate target action values for minibatch\n",
    "        target_action_value_estimates = self.q_net_target(torch.tensor(batch_state_t1).float()).detach().numpy()\n",
    "        y = batch_reward_t + self.discount_factor * self.e_greedy.select_action(target_action_value_estimates, force='exploit').values\n",
    "        \n",
    "        # Reset gradients for training step\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate action values using Q for minibatch\n",
    "        action_value_estimates = self.q_net(torch.tensor(batch_state_t).float())\n",
    "        \n",
    "        # Select action values based on the taken actions\n",
    "        x = action_value_estimates[range(len(action_value_estimates)), batch_action_t]\n",
    "        \n",
    "        # Calculate loss and update weights of Q\n",
    "        loss = self.loss_fn(x, torch.from_numpy(y))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Every target_update_interval update Q- with weights of Q\n",
    "        if self.train_step % self.target_update_interval == 0:\n",
    "            self.q_net_target.load_state_dict(self.q_net.state_dict())\n",
    "            self.q_net_target.eval()\n",
    "            \n",
    "        # Update training step\n",
    "        self.train_step += 1\n",
    "    \n",
    "    def _init_train(self):\n",
    "        # Clear memory from previous training\n",
    "        self.memory.clear()\n",
    "        \n",
    "        # Make target network, initialize its weights to match Q network weights and set it to eval mode\n",
    "        self.q_net_target = self.make_qnet()\n",
    "        self.q_net_target.load_state_dict(self.q_net.state_dict())\n",
    "        self.q_net_target.eval()\n",
    "    \n",
    "        # Loss function is MSE\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = self.Optimizer(self.q_net.parameters(), self.learning_rate)\n",
    "    \n",
    "        self.train_step = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_qnet_fn(env):\n",
    "    return lambda: DeepQNetwork(env.observation_space.shape[0], 20, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-4707718b72bb>:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output_layer_activation_fn(output_output)\n",
      "<ipython-input-2-8084a1b09446>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(list(self.memory))[indecies]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8. 10. 11. 11. 10.  8.  9. 10. 11.  8.]\n",
      "[10.  9. 10. 10. 10. 10.  9. 10.  9. 10.]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set hyper-parameters\n",
    "# Replay Memory Capacity\n",
    "N = 50\n",
    "\n",
    "train_episodes = 10000\n",
    "preview_episodes = 10\n",
    "\n",
    "batch_size = 20\n",
    "learning_rate = 0.001\n",
    "target_update_interval = 8\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Create agent\n",
    "trainParams = TrainParams(batch_size, learning_rate, target_update_interval, optim.SGD)\n",
    "policyParams = PolicyParams(EpsilonGreedyPolicy(0.1), ReplayMemory(N), 0.9, make_qnet_fn(env))\n",
    "agent = Agent(trainParams, policyParams)\n",
    "\n",
    "# Get agent baseline\n",
    "agent.set_state('eval')\n",
    "print(env_loop(env, agent, preview_episodes, render=False, max_steps_per_episode=200))\n",
    "\n",
    "# Train in env environment\n",
    "agent.set_state('train')\n",
    "env_loop(env, agent, train_episodes, render=False, max_steps_per_episode=200)\n",
    "\n",
    "# Preview learned agent\n",
    "agent.set_state('eval')\n",
    "print(env_loop(env, agent, preview_episodes, render=True, max_steps_per_episode=200))\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
