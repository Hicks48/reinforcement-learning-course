{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerp(start, end, x):\n",
    "    return (x * end) + ((1 - x) * start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.030s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class MathHelperTest(unittest.TestCase):\n",
    "    \n",
    "    def test_lerp_0(self):\n",
    "        self.assertEqual(lerp(-2, 2, 0), -2)\n",
    "    \n",
    "    def test_lerp_25(self):\n",
    "        self.assertEqual(lerp(-2, 2, 0.25), -1)\n",
    "    \n",
    "    def test_lerp_75(self):\n",
    "        self.assertEqual(lerp(-2, 2, 0.75), 1)\n",
    "        \n",
    "    def test_lerp_1(self):\n",
    "        self.assertEqual(lerp(-2, 2, 1), 2)\n",
    "        \n",
    "    def test_lerp_backwards(self):\n",
    "        self.assertEqual(lerp(2, -2, 0.25), 1)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay memory stores given `capacity` amount of transitions in a buffer. It provides methods for pushing new transitions and sampling a minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.clear()\n",
    "    \n",
    "    def clear(self):\n",
    "        self.memory = deque(maxlen=self.capacity)\n",
    "    \n",
    "    def push(self, transition):\n",
    "        self.memory.appendleft(transition)\n",
    "    \n",
    "    def sample(self, size, withReplacement=False):\n",
    "        if size > len(self.memory) and not withReplacement:\n",
    "            return None\n",
    "        \n",
    "        indecies = np.random.choice(len(self.memory), size, replace=withReplacement)\n",
    "        return np.array(list(self.memory))[indecies]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "............\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.043s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class ReplayMemoryTest(unittest.TestCase):\n",
    "    \n",
    "    def test_should_drop_oldest_memory_on_overflow(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        memory.push(3)\n",
    "        memory.push(4)\n",
    "        memory.push(5)\n",
    "        \n",
    "        self.assertEqual(list(memory.memory), [5, 4, 3, 2])\n",
    "        \n",
    "    def test_should_sample_with_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        memory.push(3)\n",
    "        memory.push(4)\n",
    "        \n",
    "        miniBatch = memory.sample(3, withReplacement=True)\n",
    "        self.assertEqual(len(miniBatch), 3)\n",
    "        \n",
    "        for elem in miniBatch:\n",
    "            self.assertIn(elem, [1, 2, 3, 4])\n",
    "            \n",
    "    def test_should_sample_without_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        memory.push(3)\n",
    "        memory.push(4)\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=False)\n",
    "        miniBatch.sort()\n",
    "        \n",
    "        self.assertEqual(miniBatch.tolist(), [1, 2, 3, 4])\n",
    "    \n",
    "    def test_should_sample_larger_batch_size_with_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=True)\n",
    "        \n",
    "        self.assertEqual(len(miniBatch), 4)\n",
    "        for elem in miniBatch:\n",
    "            self.assertIn(elem, [1, 2])\n",
    "    \n",
    "    def test_should_return_none_when_spamling_larger_batch_size_without_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=False)\n",
    "        self.assertEqual(miniBatch, None)\n",
    "    \n",
    "    def test_should_handle_memorizing_multidimensional_arrays(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push([1, 2])\n",
    "        memory.push([3, 4])\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=True)\n",
    "        \n",
    "        self.assertEqual(len(miniBatch), 4)\n",
    "        for elem in miniBatch:\n",
    "            self.assertTrue((elem == [1, 2]).all() or (elem == [3, 4]).all())\n",
    "    \n",
    "    def test_should_clear_memory(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        \n",
    "        memory.clear()\n",
    "        self.assertEqual(len(memory.memory), 0)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "ActionSelections = namedtuple('ActionSelections', ['indecies', 'values'])\n",
    "\n",
    "class EpsilonGreedyPolicy(object):\n",
    "    \n",
    "    def __init__(self, epsilon, anealing=None, random_engine=random):\n",
    "        self.epsilon = epsilon\n",
    "        self.anealing = anealing\n",
    "        self.random_engine = random_engine\n",
    "    \n",
    "    def select_action(self, action_value_functions, step=None, force=None):\n",
    "        if action_value_functions is None or len(action_value_functions) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Used at selection contains row indecies\n",
    "        row_idx = range(len(action_value_functions))\n",
    "        \n",
    "        # Find index of max column for each row\n",
    "        greedy_actions = np.argmax(action_value_functions, axis=1)\n",
    "\n",
    "        # Find non greedy actions for each row\n",
    "        non_greedy_actions = np.random.choice(len(action_value_functions[0]) - 1, len(action_value_functions))\n",
    "        non_greedy_actions += (non_greedy_actions >= greedy_actions)\n",
    "        \n",
    "        if force == 'exploit':\n",
    "            return ActionSelections(greedy_actions, np.array(action_value_functions)[row_idx, greedy_actions])\n",
    "        elif force == 'explore':\n",
    "            return ActionSelections(non_greedy_actions, np.array(action_value_functions)[row_idx, non_greedy_actions])\n",
    "        \n",
    "        limit = self.epsilon if step is None or self.anealing is None else self.anealing(step)\n",
    "        \n",
    "        if self.random_engine.uniform(0, 1) > limit:\n",
    "            return ActionSelections(greedy_actions, np.array(action_value_functions)[row_idx, greedy_actions])\n",
    "        \n",
    "        return ActionSelections(non_greedy_actions, np.array(action_value_functions)[row_idx, non_greedy_actions])\n",
    "                                                                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...................\n",
      "----------------------------------------------------------------------\n",
      "Ran 19 tests in 0.137s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "import unittest\n",
    "\n",
    "class EpsilonGreedyPolicyTest(unittest.TestCase):\n",
    "    \n",
    "    def test_should_select_greedy_when_forced(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.01)\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]], force='exploit')\n",
    "        np.testing.assert_allclose(selection.indecies, [1, 0])\n",
    "        np.testing.assert_allclose(selection.values, [0.4, 0.9])\n",
    "        \n",
    "    def test_should_select_explore_when_forced(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.01)\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]], force='explore')\n",
    "        \n",
    "        self.assertIn(selection.indecies[0], [0, 2])\n",
    "        self.assertIn(selection.indecies[1], [1, 2])\n",
    "        \n",
    "        self.assertIn(selection.values[0], [0.1, 0.2])\n",
    "        self.assertIn(selection.values[1], [0.5, 0.1])\n",
    "    \n",
    "    def test_should_select_greedy_when_random_is_higher_than_epsilon(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.1, random_engine=SimpleNamespace(uniform=lambda a, b: 0.2))\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]])\n",
    "        np.testing.assert_allclose(selection.indecies, [1, 0])\n",
    "        np.testing.assert_allclose(selection.values, [0.4, 0.9])\n",
    "    \n",
    "    def test_should_select_eploratively_when_random_is_higher_than_epsilon(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.1, random_engine=SimpleNamespace(uniform=lambda a, b: 0.05))\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]])\n",
    "        \n",
    "        self.assertIn(selection.indecies[0], [0, 2])\n",
    "        self.assertIn(selection.indecies[1], [1, 2])\n",
    "        \n",
    "        self.assertIn(selection.values[0], [0.1, 0.2])\n",
    "        self.assertIn(selection.values[1], [0.5, 0.1])\n",
    "\n",
    "    def test_should_return_none_with_empty_action_value_functions(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.01)\n",
    "        self.assertEqual(e_greedy.select_action([], force='exploit'), None)\n",
    "        \n",
    "    def test_should_use_anealing_when_step_is_provided(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.1, random_engine=SimpleNamespace(uniform=lambda a, b: 0.4), anealing=lambda step: lerp(1.0, 0.0, step / 10))\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]], step=5)\n",
    "        \n",
    "        self.assertIn(selection.indecies[0], [0, 2])\n",
    "        self.assertIn(selection.indecies[1], [1, 2])\n",
    "        \n",
    "        self.assertIn(selection.values[0], [0.1, 0.2])\n",
    "        self.assertIn(selection.values[1], [0.5, 0.1])\n",
    "    \n",
    "    def test_should_ignore_step_if_anealing_not_provided(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.1, random_engine=SimpleNamespace(uniform=lambda a, b: 0.2))\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]], step=5)\n",
    "        \n",
    "        np.testing.assert_allclose(selection.indecies, [1, 0])\n",
    "        np.testing.assert_allclose(selection.values, [0.4, 0.9])\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def env_loop(env, agent, episodes, render=False, max_steps_per_episode=None, collect_actions=False):\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    \n",
    "    for i_episode in tqdm(range(episodes), desc='episode'):\n",
    "        episode_rewards = 0\n",
    "        episode_actions = []\n",
    "        \n",
    "        # Initialize the environment and state for the episode\n",
    "        state_t = env.reset()\n",
    "        \n",
    "        i_episode_step = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done and (max_steps_per_episode is None or max_steps_per_episode > i_episode_step):\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            action_t = agent.select_action(state_t)\n",
    "            state_t1, reward_t, done, _ = env.step(action_t)\n",
    "            \n",
    "            agent.observe([state_t, action_t, reward_t, state_t1])\n",
    "            \n",
    "            episode_rewards += reward_t\n",
    "            \n",
    "            if collect_actions:\n",
    "                episode_actions.append(action_t)\n",
    "            \n",
    "            state_t = state_t1\n",
    "            i_episode_step += 1\n",
    "        \n",
    "        rewards.append(episode_rewards)\n",
    "        \n",
    "        if collect_actions:\n",
    "            actions.append(episode_actions)\n",
    "    \n",
    "    return np.array(rewards), actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_activation_fn = nn.Sigmoid()\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input_features):\n",
    "        hidden_output = self.hidden_layer(input_features)\n",
    "        hidden_activation = self.hidden_activation_fn(hidden_output)\n",
    "        \n",
    "        return self.output_layer(hidden_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....................\n",
      "----------------------------------------------------------------------\n",
      "Ran 20 tests in 0.074s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class DeepQNetworkTest(unittest.TestCase):\n",
    "    \n",
    "    def test_should_perform_forward_pass(self):\n",
    "        input_features = torch.randn(3, 4)\n",
    "        \n",
    "        deep_q = DeepQNetwork(4, 20, 5)\n",
    "        output = deep_q(input_features)\n",
    "        \n",
    "        self.assertEqual(output.size(), (3, 5))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "\n",
    "TrainParams = namedtuple('TrainParams', ['batch_size', 'learning_rate', 'target_update_interval', 'Optimizer'])\n",
    "PolicyParams = namedtuple('PolicyParams', ['e_greedy', 'memory', 'discount_factor', 'make_qnet'])\n",
    "\n",
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, trainParams, policyParams):\n",
    "        self.e_greedy = policyParams.e_greedy\n",
    "        self.q_net = policyParams.make_qnet()\n",
    "        \n",
    "        self.memory = policyParams.memory\n",
    "        self.discount_factor = policyParams.discount_factor\n",
    "        self.target_update_interval = trainParams.target_update_interval\n",
    "        \n",
    "        self.batch_size = trainParams.batch_size\n",
    "        self.learning_rate = trainParams.learning_rate\n",
    "        self.Optimizer = trainParams.Optimizer\n",
    "        \n",
    "        self.make_qnet = policyParams.make_qnet\n",
    "        \n",
    "        self.train_step = 0\n",
    "        self.state = 'eval'\n",
    "    \n",
    "    def set_state(self, state):\n",
    "        if self.state == state or state not in ['train', 'eval']:\n",
    "            return\n",
    "        \n",
    "        if state == 'train':\n",
    "            self._init_train()\n",
    "        \n",
    "        self.state = state\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        action_values = self.q_net(torch.tensor(state).float())\n",
    "        return self.e_greedy.select_action(\n",
    "            [action_values.detach().numpy()],\n",
    "            force='exploit' if self.state == 'eval' else None,\n",
    "            step=self.train_step if self.state == 'train' else None\n",
    "        ).indecies[0]\n",
    "    \n",
    "    def observe(self, transition):\n",
    "        # No need to do anything unless is training\n",
    "        if self.state != 'train':\n",
    "            return\n",
    "        \n",
    "        # Store observed transition to memory\n",
    "        self.memory.push(transition)\n",
    "        \n",
    "        # Sample minibatch from memory\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Don't train until has enough samples\n",
    "        if batch is None:\n",
    "            return\n",
    "        \n",
    "        # Unpack batch\n",
    "        [batch_state_t, batch_action_t, batch_reward_t, batch_state_t1] = batch.T\n",
    "        batch_state_t = np.concatenate(batch_state_t).reshape((-1, 4))\n",
    "        batch_action_t = batch_action_t.astype(np.int32, copy=False)\n",
    "        batch_reward_t = batch_reward_t.astype(np.float32, copy=False)\n",
    "        batch_state_t1 = np.concatenate(batch_state_t1).reshape((-1, 4))\n",
    "        \n",
    "        # Calculate target action values for minibatch\n",
    "        target_action_value_estimates = self.q_net_target(torch.tensor(batch_state_t1).float()).detach().numpy()\n",
    "        y = batch_reward_t + self.discount_factor * self.e_greedy.select_action(target_action_value_estimates, force='exploit').values\n",
    "        \n",
    "        # Reset gradients for training step\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate action values using Q for minibatch\n",
    "        action_value_estimates = self.q_net(torch.tensor(batch_state_t).float())\n",
    "        \n",
    "        # Select action values based on the taken actions\n",
    "        x = action_value_estimates[range(len(action_value_estimates)), batch_action_t]\n",
    "        \n",
    "        # Calculate loss and update weights of Q\n",
    "        loss = self.loss_fn(x, torch.from_numpy(y))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Every target_update_interval update Q- with weights of Q\n",
    "        if self.train_step % self.target_update_interval == 0:\n",
    "            self.q_net_target.load_state_dict(self.q_net.state_dict())\n",
    "            self.q_net_target.eval()\n",
    "            \n",
    "        # Update training step\n",
    "        self.train_step += 1\n",
    "    \n",
    "    def _init_train(self):\n",
    "        # Clear memory from previous training\n",
    "        self.memory.clear()\n",
    "        \n",
    "        # Make target network, initialize its weights to match Q network weights and set it to eval mode\n",
    "        self.q_net_target = self.make_qnet()\n",
    "        self.q_net_target.load_state_dict(self.q_net.state_dict())\n",
    "        self.q_net_target.eval()\n",
    "    \n",
    "        # Loss function is MSE\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = self.Optimizer(self.q_net.parameters(), self.learning_rate)\n",
    "    \n",
    "        self.train_step = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_qnet_fn(env):\n",
    "    return lambda: DeepQNetwork(env.observation_space.shape[0], 20, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79da4751d43a4432b9447f38ab9f0da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='episode'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "baseline actions: [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "baseline rewards: [ 9.  9. 10.  9.  9. 10. 10. 10. 10. 10.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d8d59cc17e44a38f29504cbd0b9e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='episode'), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-8084a1b09446>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(list(self.memory))[indecies]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23ce5a70e43496eac0e78d01a0361a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='episode'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "preview rewards: [10.  9.  9.  9.  9. 10. 10.  9.  8.  9.]\n",
      "preview actions: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set hyper-parameters\n",
    "# Replay Memory Capacity\n",
    "N = 100\n",
    "\n",
    "train_episodes = 10000\n",
    "preview_episodes = 10\n",
    "discount_rate = 0.9\n",
    "batch_size = 12\n",
    "learning_rate = 0.01\n",
    "target_update_interval = 8\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Create agent\n",
    "trainParams = TrainParams(\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "    target_update_interval,\n",
    "    optim.RMSprop\n",
    ")\n",
    "\n",
    "policyParams = PolicyParams(\n",
    "    EpsilonGreedyPolicy(0.1, anealing=lambda step: max(lerp(1.0, 0.1, step / 5000), 0.1)),\n",
    "    ReplayMemory(N),\n",
    "    discount_rate,\n",
    "    make_qnet_fn(env)\n",
    ")\n",
    "\n",
    "agent = Agent(trainParams, policyParams)\n",
    "\n",
    "# Get agent baseline\n",
    "agent.set_state('eval')\n",
    "baseline_rewards, baseline_actions = env_loop(env, agent, preview_episodes, render=False, max_steps_per_episode=200, collect_actions=True)\n",
    "print(f'baseline actions: {baseline_actions}')\n",
    "print(f'baseline rewards: {baseline_rewards}')\n",
    "\n",
    "# Train in env environment\n",
    "agent.set_state('train')\n",
    "train_rewards, train_actions = env_loop(env, agent, train_episodes, render=False, max_steps_per_episode=200)\n",
    "\n",
    "# Visualize training\n",
    "plt.plot(list(range(len(train_rewards))), train_rewards)\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('reward')\n",
    "plt.show()\n",
    "\n",
    "# Preview learned agent\n",
    "agent.set_state('eval')\n",
    "preview_rewards, preview_actions = env_loop(env, agent, preview_episodes, render=True, max_steps_per_episode=200, collect_actions=True)\n",
    "print(f'preview rewards: {preview_rewards}')\n",
    "print(f'preview actions: {preview_actions}')\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Agents Q-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "save_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "torch.save(agent.q_net, os.path.join(save_dir, 'saves', 'qnet.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Agent with Q-Model for evaluation only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "agent = Agent(Agent(trainParams, policyParams), policyParams)\n",
    "load_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "agent.q_net.load_state_dict(torch.load(os.path.join(load_dir, 'saves', 'qnet.pt')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
