{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay memory stores given `capacity` amount of transitions in a buffer. It provides methods for pushing new transitions and sampling a minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.clear()\n",
    "    \n",
    "    def clear(self):\n",
    "        self.memory = deque(maxlen=self.capacity)\n",
    "    \n",
    "    def push(self, transition):\n",
    "        self.memory.appendleft(transition)\n",
    "    \n",
    "    def sample(self, size, withReplacement=False):\n",
    "        if size > len(self.memory) and not withReplacement:\n",
    "            return None\n",
    "        \n",
    "        indecies = np.random.choice(len(self.memory), size, replace=withReplacement)\n",
    "        return np.array(list(self.memory))[indecies]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.025s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class ReplayMemoryTest(unittest.TestCase):\n",
    "    \n",
    "    def test_should_drop_oldest_memory_on_overflow(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        memory.push(3)\n",
    "        memory.push(4)\n",
    "        memory.push(5)\n",
    "        \n",
    "        self.assertEqual(list(memory.memory), [5, 4, 3, 2])\n",
    "        \n",
    "    def test_should_sample_with_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        memory.push(3)\n",
    "        memory.push(4)\n",
    "        \n",
    "        miniBatch = memory.sample(3, withReplacement=True)\n",
    "        self.assertEqual(len(miniBatch), 3)\n",
    "        \n",
    "        for elem in miniBatch:\n",
    "            self.assertIn(elem, [1, 2, 3, 4])\n",
    "            \n",
    "    def test_should_sample_without_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        memory.push(3)\n",
    "        memory.push(4)\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=False)\n",
    "        miniBatch.sort()\n",
    "        \n",
    "        self.assertEqual(miniBatch.tolist(), [1, 2, 3, 4])\n",
    "    \n",
    "    def test_should_sample_larger_batch_size_with_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=True)\n",
    "        \n",
    "        self.assertEqual(len(miniBatch), 4)\n",
    "        for elem in miniBatch:\n",
    "            self.assertIn(elem, [1, 2])\n",
    "    \n",
    "    def test_should_return_none_when_spamling_larger_batch_size_without_replacement(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=False)\n",
    "        self.assertEqual(miniBatch, None)\n",
    "    \n",
    "    def test_should_handle_memorizing_multidimensional_arrays(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push([1, 2])\n",
    "        memory.push([3, 4])\n",
    "        \n",
    "        miniBatch = memory.sample(4, withReplacement=True)\n",
    "        \n",
    "        self.assertEqual(len(miniBatch), 4)\n",
    "        for elem in miniBatch:\n",
    "            self.assertTrue((elem == [1, 2]).all() or (elem == [3, 4]).all())\n",
    "    \n",
    "    def test_should_clear_memory(self):\n",
    "        memory = ReplayMemory(4)\n",
    "        memory.push(1)\n",
    "        memory.push(2)\n",
    "        \n",
    "        memory.clear()\n",
    "        self.assertEqual(len(memory.memory), 0)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "ActionSelections = namedtuple('ActionSelections', ['indecies', 'values'])\n",
    "\n",
    "class EpsilonGreedyPolicy(object):\n",
    "    \n",
    "    def __init__(self, epsilon, random_engine=random):\n",
    "        self.epsilon = epsilon\n",
    "        self.random_engine = random_engine\n",
    "    \n",
    "    def select_action(self, action_value_functions, force=None):\n",
    "        if action_value_functions is None or len(action_value_functions) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Used at selection contains row indecies\n",
    "        row_idx = range(len(action_value_functions))\n",
    "        \n",
    "        # Find index of max column for each row\n",
    "        greedy_actions = np.argmax(action_value_functions, axis=1)\n",
    "\n",
    "        # Find non greedy actions for each row\n",
    "        non_greedy_actions = np.random.choice(len(action_value_functions[0]) - 1, len(action_value_functions))\n",
    "        non_greedy_actions += (non_greedy_actions >= greedy_actions)\n",
    "        \n",
    "        if force == 'exploit':\n",
    "            return ActionSelections(greedy_actions, np.array(action_value_functions)[row_idx, greedy_actions])\n",
    "        elif force == 'explore':\n",
    "            return ActionSelections(non_greedy_actions, np.array(action_value_functions)[row_idx, non_greedy_actions])\n",
    "        \n",
    "        if self.random_engine.uniform(0, 1) > self.epsilon:\n",
    "            return ActionSelections(greedy_actions, np.array(action_value_functions)[row_idx, greedy_actions])\n",
    "        \n",
    "        return ActionSelections(non_greedy_actions, np.array(action_value_functions)[row_idx, non_greedy_actions])\n",
    "                                                                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "............\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.091s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "import unittest\n",
    "\n",
    "class EpsilonGreedyPolicyTest(unittest.TestCase):\n",
    "    \n",
    "    def test_should_select_greedy_when_forced(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.01)\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]], force='exploit')\n",
    "        np.testing.assert_allclose(selection.indecies, [1, 0])\n",
    "        np.testing.assert_allclose(selection.values, [0.4, 0.9])\n",
    "        \n",
    "    def test_should_select_explore_when_forced(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.01)\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]], force='explore')\n",
    "        \n",
    "        self.assertIn(selection.indecies[0], [0, 2])\n",
    "        self.assertIn(selection.indecies[1], [1, 2])\n",
    "        \n",
    "        self.assertIn(selection.values[0], [0.1, 0.2])\n",
    "        self.assertIn(selection.values[1], [0.5, 0.1])\n",
    "    \n",
    "    def test_should_select_greedy_when_random_is_higher_than_epsilon(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.1, random_engine=SimpleNamespace(uniform=lambda a, b: 0.2))\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]])\n",
    "        np.testing.assert_allclose(selection.indecies, [1, 0])\n",
    "        np.testing.assert_allclose(selection.values, [0.4, 0.9])\n",
    "    \n",
    "    def test_should_select_eploratively_when_random_is_higher_than_epsilon(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.1, random_engine=SimpleNamespace(uniform=lambda a, b: 0.05))\n",
    "        selection = e_greedy.select_action([[0.1, 0.4, 0.2], [0.9, 0.5, 0.1]])\n",
    "        \n",
    "        self.assertIn(selection.indecies[0], [0, 2])\n",
    "        self.assertIn(selection.indecies[1], [1, 2])\n",
    "        \n",
    "        self.assertIn(selection.values[0], [0.1, 0.2])\n",
    "        self.assertIn(selection.values[1], [0.5, 0.1])\n",
    "\n",
    "    def test_should_return_none_with_empty_action_value_functions(self):\n",
    "        e_greedy = EpsilonGreedyPolicy(0.01)\n",
    "        self.assertEqual(e_greedy.select_action([], force='exploit'), None)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def env_loop(env, agent, episodes, render=False, max_steps_per_episode=None):\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    \n",
    "    for i_episode in tqdm(range(episodes), desc='episode'):\n",
    "        episode_rewards = 0\n",
    "        episode_actions = []\n",
    "        \n",
    "        # Initialize the environment and state for the episode\n",
    "        state_t = env.reset()\n",
    "        \n",
    "        i_episode_step = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done and (max_steps_per_episode is None or max_steps_per_episode > i_episode_step):\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            action_t = agent.select_action(state_t)\n",
    "            state_t1, reward_t, done, _ = env.step(action_t)\n",
    "            \n",
    "            agent.observe([state_t, action_t, reward_t, state_t1])\n",
    "            \n",
    "            episode_rewards += reward_t\n",
    "            episode_actions.append(action_t)\n",
    "            \n",
    "            state_t = state_t1\n",
    "            i_episode_step += 1\n",
    "        \n",
    "        rewards.append(episode_rewards)\n",
    "        actions.append(episode_actions)\n",
    "    \n",
    "    return np.array(rewards), actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_activation_fn = nn.Sigmoid()\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.output_layer_activation_fn = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_features):\n",
    "        hidden_output = self.hidden_layer(input_features)\n",
    "        hidden_activation = self.hidden_activation_fn(hidden_output)\n",
    "        \n",
    "        output_output = self.output_layer(hidden_activation)\n",
    "        return self.output_layer_activation_fn(output_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".............\n",
      "----------------------------------------------------------------------\n",
      "Ran 13 tests in 0.037s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class DeepQNetworkTest(unittest.TestCase):\n",
    "    \n",
    "    def test_should_perform_forward_pass(self):\n",
    "        input_features = torch.randn(3, 4)\n",
    "        \n",
    "        deep_q = DeepQNetwork(4, 20, 5)\n",
    "        output = deep_q(input_features)\n",
    "        \n",
    "        self.assertEqual(output.size(), (3, 5))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "\n",
    "TrainParams = namedtuple('TrainParams', ['batch_size', 'learning_rate', 'target_update_interval', 'Optimizer'])\n",
    "PolicyParams = namedtuple('PolicyParams', ['e_greedy', 'memory', 'discount_factor', 'make_qnet'])\n",
    "\n",
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, trainParams, policyParams):\n",
    "        self.e_greedy = policyParams.e_greedy\n",
    "        self.q_net = policyParams.make_qnet()\n",
    "        \n",
    "        self.memory = policyParams.memory\n",
    "        self.discount_factor = policyParams.discount_factor\n",
    "        self.target_update_interval = trainParams.target_update_interval\n",
    "        \n",
    "        self.batch_size = trainParams.batch_size\n",
    "        self.learning_rate = trainParams.learning_rate\n",
    "        self.Optimizer = trainParams.Optimizer\n",
    "        \n",
    "        self.make_qnet = policyParams.make_qnet\n",
    "        \n",
    "        self.state = 'eval'\n",
    "    \n",
    "    def set_state(self, state):\n",
    "        if self.state == state or state not in ['train', 'eval']:\n",
    "            return\n",
    "        \n",
    "        if state == 'train':\n",
    "            self._init_train()\n",
    "        \n",
    "        self.state = state\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        action_values = self.q_net(torch.tensor(state).float())\n",
    "        action = self.e_greedy.select_action([action_values.detach().numpy()], force='exploit' if self.state == 'eval' else None).indecies[0]\n",
    "        return action\n",
    "    \n",
    "    def observe(self, transition):\n",
    "        # No need to do anything unless is training\n",
    "        if self.state != 'train':\n",
    "            return\n",
    "        \n",
    "        # Store observed transition to memory\n",
    "        self.memory.push(transition)\n",
    "        \n",
    "        # Sample minibatch from memory\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Don't train until has enough samples\n",
    "        if batch is None:\n",
    "            return\n",
    "        \n",
    "        # Unpack batch\n",
    "        [batch_state_t, batch_action_t, batch_reward_t, batch_state_t1] = batch.T\n",
    "        batch_state_t = np.concatenate(batch_state_t).reshape((-1, 4))\n",
    "        batch_action_t = batch_action_t.astype(np.int32, copy=False)\n",
    "        batch_reward_t = batch_reward_t.astype(np.float32, copy=False)\n",
    "        batch_state_t1 = np.concatenate(batch_state_t1).reshape((-1, 4))\n",
    "        \n",
    "        # Calculate target action values for minibatch\n",
    "        target_action_value_estimates = self.q_net_target(torch.tensor(batch_state_t1).float()).detach().numpy()\n",
    "        y = batch_reward_t + self.discount_factor * self.e_greedy.select_action(target_action_value_estimates, force='exploit').values\n",
    "        \n",
    "        # Reset gradients for training step\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate action values using Q for minibatch\n",
    "        action_value_estimates = self.q_net(torch.tensor(batch_state_t).float())\n",
    "        \n",
    "        # Select action values based on the taken actions\n",
    "        x = action_value_estimates[range(len(action_value_estimates)), batch_action_t]\n",
    "        \n",
    "        # Calculate loss and update weights of Q\n",
    "        loss = self.loss_fn(x, torch.from_numpy(y))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Every target_update_interval update Q- with weights of Q\n",
    "        if self.train_step % self.target_update_interval == 0:\n",
    "            self.q_net_target.load_state_dict(self.q_net.state_dict())\n",
    "            self.q_net_target.eval()\n",
    "            \n",
    "        # Update training step\n",
    "        self.train_step += 1\n",
    "    \n",
    "    def _init_train(self):\n",
    "        # Clear memory from previous training\n",
    "        self.memory.clear()\n",
    "        \n",
    "        # Make target network, initialize its weights to match Q network weights and set it to eval mode\n",
    "        self.q_net_target = self.make_qnet()\n",
    "        self.q_net_target.load_state_dict(self.q_net.state_dict())\n",
    "        self.q_net_target.eval()\n",
    "    \n",
    "        # Loss function is MSE\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = self.Optimizer(self.q_net.parameters(), self.learning_rate)\n",
    "    \n",
    "        self.train_step = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_qnet_fn(env):\n",
    "    return lambda: DeepQNetwork(env.observation_space.shape[0], 20, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52377a6e483d488bb234059ee1e6b126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='episode'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "baseline actions: [[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1], [1, 1, 1, 1, 1, 0, 1, 0, 1, 0], [1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1], [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0], [1, 1, 1, 1, 1, 0, 1, 0, 1, 0], [1, 1, 1, 1, 1, 1, 0, 1, 0, 0], [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0], [1, 1, 1, 1, 1, 0, 1, 0, 1, 0], [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1], [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1]]\n",
      "baseline rewards: [11. 10. 11. 12. 10. 10. 11. 10. 11. 11.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a229509bd244b1cbf36dbc81bc27f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='episode'), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-8084a1b09446>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(list(self.memory))[indecies]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d0b6166687430692ea8eb777dad62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='episode'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "preview rewards: [ 9.  9.  8.  9. 10.  8. 10. 10. 10.  9.]\n",
      "preview actions: [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set hyper-parameters\n",
    "# Replay Memory Capacity\n",
    "N = 50\n",
    "\n",
    "train_episodes = 10000\n",
    "preview_episodes = 10\n",
    "\n",
    "batch_size = 20\n",
    "learning_rate = 0.001\n",
    "target_update_interval = 8\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Create agent\n",
    "trainParams = TrainParams(batch_size, learning_rate, target_update_interval, optim.SGD)\n",
    "policyParams = PolicyParams(EpsilonGreedyPolicy(0.1), ReplayMemory(N), 0.9, make_qnet_fn(env))\n",
    "agent = Agent(trainParams, policyParams)\n",
    "\n",
    "# Get agent baseline\n",
    "agent.set_state('eval')\n",
    "baseline_rewards, baseline_actions = env_loop(env, agent, preview_episodes, render=False, max_steps_per_episode=200)\n",
    "print(f'baseline actions: {baseline_actions}')\n",
    "print(f'baseline rewards: {baseline_rewards}')\n",
    "\n",
    "# Train in env environment\n",
    "agent.set_state('train')\n",
    "train_rewards, train_actions = env_loop(env, agent, train_episodes, render=False, max_steps_per_episode=200)\n",
    "\n",
    "# Visualize training\n",
    "plt.plot(list(range(len(train_rewards))), train_rewards)\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('reward')\n",
    "plt.show()\n",
    "\n",
    "# Preview learned agent\n",
    "agent.set_state('eval')\n",
    "preview_rewards, preview_actions = env_loop(env, agent, preview_episodes, render=True, max_steps_per_episode=200)\n",
    "print(f'preview rewards: {preview_rewards}')\n",
    "print(f'preview actions: {preview_actions}')\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
